from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import ipfshttpclient
import json
from transformers import pipeline
from bertopic import BERTopic
import datetime
from typing import List, Dict
import random
import logging
from trivia_templates import QUESTION_TEMPLATES

# Set random seed for reproducible nickname/motto during testing
random.seed(42)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# Pydantic models
class Tweet(BaseModel):
    text: str
    created_at: str

class GenerateRequest(BaseModel):
    username: str
    tweets: List[Tweet]

# Initialize NLP models
try:
    sentiment_analyzer = pipeline("text-classification", model="nlptown/bert-base-multilingual-uncased-sentiment")
    logger.info("Sentiment analyzer loaded successfully.")
except Exception as e:
    logger.error(f"Failed to load sentiment analyzer: {e}")
    raise

# IPFS client (deferred to Phase 3 for full fix)
try:
    ipfs_client = ipfshttpclient.connect('/ip4/127.0.0.1/tcp/5001')
    logger.info("IPFS client connected successfully.")
except Exception as e:
    logger.warning(f"IPFS connection failed: {e}. Using local storage fallback.")
    ipfs_client = None

SENTIMENT_TO_TONE = {
    "1 star": "deep",
    "2 stars": "chaotic",
    "3 stars": "chill",
    "4 stars": "happy",
    "5 stars": "happy"
}

def score_big_five(tweets: List[Dict]) -> Dict[str, float]:
    openness = 50.0
    conscientiousness = 50.0
    extraversion = 50.0
    agreeableness = 50.0
    neuroticism = 50.0
    tweet_count = len(tweets)
    
    if not tweets:
        return {
            "Openness": openness,
            "Conscientiousness": conscientiousness,
            "Extraversion": extraversion,
            "Agreeableness": agreeableness,
            "Neuroticism": neuroticism
        }

    topics = analyze_topics(tweets)
    logger.info(f"Topics detected: {topics}")
    
    # Batch sentiment analysis
    texts = [tweet.get("text", "") for tweet in tweets if tweet.get("text", "")]
    sentiments = sentiment_analyzer(texts, batch_size=8) if texts else [{"label": "3 stars"}]
    
    # Normalize scores by % of tweets
    openness_tweets = sum(1 for t in tweets if any(w in t.get("text", "").lower() for w in ["innovate", "create", "tech", "hackathon", "web3"]))
    conscientiousness_tweets = sum(1 for t in tweets if any(w in t.get("text", "").lower() for w in ["work", "project", "deadline", "plan"]))
    extraversion_tweets = sum(1 for t in tweets if any(w in t.get("text", "").lower() for w in ["party", "meetup", "vibe", "friends"]))
    agreeableness_tweets = sum(1 for t, s in zip(tweets, sentiments) if s["label"] in ["4 stars", "5 stars"] or "thanks" in t.get("text", "").lower())
    neuroticism_tweets = sum(1 for t, s in zip(tweets, sentiments) if s["label"] in ["1 star", "2 stars"])

    openness += 10 * (openness_tweets / tweet_count)
    conscientiousness += 10 * (conscientiousness_tweets / tweet_count)
    extraversion += 10 * (extraversion_tweets / tweet_count)
    agreeableness += 10 * (agreeableness_tweets / tweet_count)
    neuroticism += 10 * (neuroticism_tweets / tweet_count)

    # Topic-based adjustments
    if "Web3" in topics:
        openness += 10 * (sum(1 for t in tweets if "web3" in t.get("text", "").lower()) / tweet_count)
    if "AI" in topics:
        conscientiousness += 5 * (sum(1 for t in tweets if "ai" in t.get("text", "").lower()) / tweet_count)

    # Log sentiment for debugging
    for tweet, sentiment in zip(tweets, sentiments):
        text = tweet.get("text", "")
        sentiment_label = sentiment["label"] if text else "3 stars"
        logger.info(f"Tweet: {text[:50]}... Sentiment: {sentiment_label}")

    return {
        "Openness": min(max(openness, 0), 100),
        "Conscientiousness": min(max(conscientiousness, 0), 100),
        "Extraversion": min(max(extraversion, 0), 100),
        "Agreeableness": min(max(agreeableness, 0), 100),
        "Neuroticism": min(max(neuroticism, 0), 100)
    }

def analyze_posting_behavior(tweets: List[Dict]) -> str:
    try:
        hours = [datetime.datetime.fromisoformat(t.get("created_at", "2025-01-01T00:00:00Z").replace("Z", "+00:00")).hour for t in tweets]
        if not hours:
            return "Casual tweeter"
        bins = {"Morning": 0, "Afternoon": 0, "Night": 0}
        for h in hours:
            if 6 <= h < 12:
                bins["Morning"] += 1
            elif 12 <= h < 18:
                bins["Afternoon"] += 1
            else:
                bins["Night"] += 1
        max_bin = max(bins, key=bins.get)
        logger.info(f"Posting bins: {bins}")
        return f"{max_bin} tweeter"
    except Exception as e:
        logger.warning(f"Posting behavior analysis failed: {e}")
        return "Casual tweeter"

def analyze_writing_style(tweets: List[Dict]) -> str:
    meme_count = sum(1 for tweet in tweets if "#" in tweet.get("text", "") or "lol" in tweet.get("text", "").lower())
    thread_count = sum(1 for tweet in tweets if len(tweet.get("text", "")) > 100)
    if meme_count > len(tweets) / 2:
        return "Meme lord"
    elif thread_count > len(tweets) / 2:
        return "Thread master"
    return "Casual tweeter"

def keyword_based_topics(tweets: List[Dict]) -> List[str]:
    topic_counts = {"Web3": 0, "AI": 0}
    for tweet in tweets:
        text = tweet.get("text", "").lower()
        if any(word in text for word in ["web3", "blockchain", "hackathon"]):
            topic_counts["Web3"] += 1
        if any(word in text for word in ["ai", "bert", "model"]):
            topic_counts["AI"] += 1
    topics = [topic for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True) if count > 0]
    return topics or ["General"]

def analyze_topics(tweets: List[Dict]) -> List[str]:
    texts = [tweet.get("text", "") for tweet in tweets]
    try:
        topic_model = BERTopic(min_topic_size=2, embedding_model="all-MiniLM-L6-v2")
        topics, _ = topic_model.fit_transform(texts)
        topic_info = topic_model.get_topic_info()
        topic_names = [row["Name"] for row in topic_info.to_dict("records") if row["Topic"] != -1]
        if any("web3" in name.lower() or "blockchain" in name.lower() for name in topic_names):
            logger.info(f"BERTopic detected topics: {topic_names}")
            return ["Web3"] + [name for name in topic_names if "web3" not in name.lower()]
        logger.info(f"BERTopic detected topics: {topic_names}")
        return topic_names or ["General"]
    except Exception as e:
        logger.warning(f"BERTopic failed: {e}. Falling back to keyword-based.")
        return keyword_based_topics(tweets)

def generate_personality_report(username: str, tweets: List[Dict]) -> Dict:
    logger.info(f"Generating personality report for {username}")
    big5 = score_big_five(tweets)
    posting_style = analyze_posting_behavior(tweets)
    writing_style = analyze_writing_style(tweets)
    topics = analyze_topics(tweets)

    nickname = "The " + random.choice(["Wild", "Cosmic", "Tech"]) + " " + random.choice(["Trailblazer", "Philosopher", "VibeMaster"])
    motto = random.choice([
        "Tweet hard, vibe easy!",
        "Code by day, memes by night!",
        "Building the future, one tweet at a time."
    ])
    superpower = random.choice([
        "Turning chaos into viral tweets",
        "Dropping Web3 wisdom in 280 characters",
        "Memes that spark revolutions"
    ])
    quirk = random.choice([
        "Tweets haikus when stressed",
        "Obsessed with late-night taco runs",
        "Has a secret meme stash"
    ])

    report = {
        "username": username,
        "big5_traits": big5,
        "nickname": nickname,
        "posting_style": f"{posting_style} {writing_style}",
        "motto": motto,
        "superpower": superpower,
        "quirk": quirk,
        "ipfs_hash": None  # Deferred to Phase 3
    }

    try:
        report_json = json.dumps(report)
        ipfs_result = ipfs_client.add_str(report_json)
        if isinstance(ipfs_result, list):
            report["ipfs_hash"] = ipfs_result[0]["Hash"]
        else:
            report["ipfs_hash"] = ipfs_result["Hash"]
        logger.info(f"Personality report saved to IPFS: {report['ipfs_hash']}")
    except Exception as e:
        logger.warning(f"IPFS upload failed for personality report: {e}. Saving locally.")
        with open(f"{username}_personality.json", "w") as f:
            json.dump(report, f)

    return report

def generate_trivia_questions(username: str, report: Dict, tweets: List[Dict]) -> Dict:
    logger.info(f"Generating 15 trivia questions for {username}")
    questions = []
    topics = analyze_topics(tweets)
    primary_topic = topics[0] if topics else "General"
    secondary_topic = topics[1] if len(topics) > 1 else "General"
    has_web3 = "Web3" in topics

    # Ensure 2â€“3 topic questions
    topic_templates = [t for t in QUESTION_TEMPLATES if t["category"] == "Topics"]
    other_templates = [t for t in QUESTION_TEMPLATES if t["category"] != "Topics"]
    selected_templates = random.sample(topic_templates, min(2, len(topic_templates))) + \
                        random.sample(other_templates, 15 - min(2, len(topic_templates)))

    for i, template in enumerate(selected_templates, 1):
        correct_answer = template["correctAnswer"]
        if callable(correct_answer):
            correct_answer = correct_answer(report)
        elif correct_answer is None:
            correct_answer = 0 if has_web3 else 1  # Default for Web3

        question = {
            "questionText": template["questionText"].format(**report, topic=primary_topic, secondary_topic=secondary_topic),
            "options": [
                opt.format(**report, topic=primary_topic, secondary_topic=secondary_topic) if isinstance(opt, str) else opt
                for opt in template["options"]
            ],
            "correctAnswer": correct_answer,
            "category": template["category"],
            "stake_amount": 0.003,
            "reward": 0.0,
            "questionId": i,
            "stage": (i - 1) // 5 + 1
        }
        questions.append(question)

    categories = {
        "Vibe": sum(1 for q in questions if q["category"] == "Vibe"),
        "Posting Habits": sum(1 for q in questions if q["category"] == "Posting Habits"),
        "Topics": sum(1 for q in questions if q["category"] == "Topics"),
        "Personality": sum(1 for q in questions if q["category"] == "Personality")
    }

    trivia = {
        "username": username,
        "questions": questions,
        "categories": categories,
        "ipfs_hash": None  # Deferred to Phase 3
    }

    try:
        trivia_json = json.dumps(trivia)
        ipfs_result = ipfs_client.add_str(trivia_json)
        if isinstance(ipfs_result, list):
            trivia["ipfs_hash"] = ipfs_result[0]["Hash"]
        else:
            trivia["ipfs_hash"] = ipfs_result["Hash"]
        logger.info(f"Trivia questions saved to IPFS: {trivia['ipfs_hash']}")
    except Exception as e:
        logger.warning(f"IPFS upload failed for trivia: {e}. Saving locally.")
        with open(f"{username}_trivia.json", "w") as f:
            json.dump(trivia, f)

    return trivia

@app.get("/testTweets/{username}")
async def get_test_tweets(username: str):
    logger.info(f"Generating mock tweets for {username}")
    mock_tweets = [
        {
            "text": "Just attended a Base hackathon! Loved the Web3 vibe.",
            "created_at": "2025-04-20T14:30:00Z"
        },
        {
            "text": "Working on a new AI model with DistilBERT. #AI #Web3",
            "created_at": "2025-04-19T09:15:00Z"
        },
        {
            "text": "Late-night coding session, fueled by tacos! ðŸŒ®",
            "created_at": "2025-04-18T02:45:00Z"
        }
    ]
    return {"username": username, "tweets": mock_tweets}

@app.post("/generatePersonalityAndQuestions")
async def generate_personality_and_questions(request: GenerateRequest):
    try:
        if not request.tweets or len(request.tweets) > 50:
            raise HTTPException(status_code=400, detail="Provide 1â€“50 tweets")

        tweets = [{"text": tweet.text, "created_at": tweet.created_at} for tweet in request.tweets]
        logger.info(f"Processing {len(tweets)} tweets for {request.username}")

        personality_report = generate_personality_report(request.username, tweets)
        trivia = generate_trivia_questions(request.username, personality_report, tweets)

        return {
            "personality_report": personality_report,
            "trivia": trivia
        }
    except Exception as e:
        logger.error(f"Error in generatePersonalityAndQuestions: {e}")
        raise HTTPException(status_code=500, detail=str(e))